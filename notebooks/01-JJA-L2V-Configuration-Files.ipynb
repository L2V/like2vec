{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML files: config and params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to files to specify configuration (location of files) and parameters of the pipeline. For this we will be using '.yml' files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load CONFIGS/ex1-ml-1m-config.yml\n",
    "JARS:\n",
    "    LLR_JAR: \"llr-assembly-1.2.jar\"\n",
    "    EMBEDDINGS_JAR: \"n2v-assembly-3.7.jar\"\n",
    "    PREDICTIONS_JAR: \"prediction-assembly-2.2.jar\"\n",
    "    EVALUATION_JAR: \"evaluation-assembly-1.1.jar\"\n",
    "PATHS:\n",
    "    JARS: \"s3://sandbox-l2v/JARs/\"\n",
    "    OUTPUT: \"s3://sandbox-l2v/datasets/ml-1m/\"\n",
    "DATA:\n",
    "    TRAIN: \"s3://sandbox-l2v/datasets/ml-1m/split/split-cleaned-formatted-4and5/ml1m-train-clean4and5\"\n",
    "    VALIDATION: \"s3://sandbox-l2v/datasets/ml-1m/split/split-cleaned-formatted/ml1m-validation-clean\"\n",
    "    TRAIN-VALIDATION: \"\"\n",
    "    TEST: \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This contains the location of the files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load CONFIGS/ex3420-du05d100w10l80n10d30p5q1-900-072717-params.yml\n",
    "EMBEDDINGS:\n",
    "  degree: 30\n",
    "  dim: 100\n",
    "  numWalks: 10\n",
    "  p: 5\n",
    "  q: 1\n",
    "  walkLength: 80\n",
    "  window: 10\n",
    "EVALUATION:\n",
    "  metric: RMSE\n",
    "LLR:\n",
    "  options: default\n",
    "  threshold: 0.5\n",
    "  useroritem: user\n",
    "PREDICTIONS:\n",
    "  neighbors: 900\n",
    "  ntype: KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This contains the hyperparameters of the L2V pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Configuration Files to AWS commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load the parameters in the two configuration files `ex1-ml-1m-config.yml` and `CONFIGS/ex3420-du05d100w10l80n10d30p5q1-900-072717-params.yml` and outputs **AWS CLI commands** which can be used to run the different steps of the pipeline in EMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_configs_params(config_yml_path, params_yml_path):\n",
    "    with open(config_yml_path, 'r') as config_ymlfile:\n",
    "        l2v_cfg = yaml.load(config_ymlfile)\n",
    "    with open(params_yml_path, 'r') as params_ymlfile:\n",
    "        l2v_params = yaml.load(params_ymlfile)\n",
    "    return l2v_cfg, l2v_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_LLR_cli(l2v_params, l2v_cfg):\n",
    "    today = datetime.datetime.now().strftime(\"%m%d%y\")\n",
    "    \n",
    "    llr_JAR = l2v_cfg['PATHS'][\"JARS\"] + l2v_cfg['JARS'][\"LLR_JAR\"]\n",
    "    jar_serialization = l2v_cfg['JARS']['LLR_JAR'].replace(\"-assembly-\",\"\").replace(\".jar\", \"\").replace(\".\",\"\")\n",
    "    llr_params_serialization = l2v_params['LLR']['options'][0] + l2v_params['LLR']['useroritem'][0] + str(l2v_params['LLR']['threshold']).replace(\".\",\"\")\n",
    "    output_folder = jar_serialization + \"-\" + today +  \"-\" + llr_params_serialization\n",
    "    output_for_llr = l2v_cfg['PATHS'][\"OUTPUT\"] + \"llr_output/\" + output_folder\n",
    "    LLR_EMR = \"\"\"spark-submit --deploy-mode cluster --class llr.LLR {} --master yarn --options {} --useroritem {} --threshold {} --interactionsFile {} --outputFile {} --separator \",\" --maxInteractionsPerUserOrItem 500 --seed 12345\"\"\".format(llr_JAR, l2v_params['LLR']['options'], l2v_params['LLR']['useroritem'], l2v_params['LLR']['threshold'], l2v_cfg['DATA']['TRAIN'], output_for_llr )\n",
    "    \n",
    "    return LLR_EMR, output_for_llr, llr_params_serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_EMB_cli(l2v_params, l2v_cfg, output_for_llr, llr_params_serialization):\n",
    "    today = datetime.datetime.now().strftime(\"%m%d%y\")\n",
    "    \n",
    "    input_embeddings = output_for_llr + \"/part-00000\"\n",
    "    ne_jar = l2v_cfg['JARS'][\"EMBEDDINGS_JAR\"].replace(\"n2v-assembly-\",\"\").replace(\".\",\"\").replace(\"jar\",\"\")\n",
    "    embs = \"d{}w{}l{}n{}d{}-\".format(l2v_params['EMBEDDINGS']['dim'], l2v_params['EMBEDDINGS']['window'], l2v_params['EMBEDDINGS']['walkLength'], l2v_params['EMBEDDINGS']['numWalks'] , l2v_params['EMBEDDINGS']['degree'])\n",
    "    n2v = \"p{}q{}\".format(l2v_params['EMBEDDINGS']['p'], l2v_params['EMBEDDINGS']['q'])\n",
    "    \n",
    "    ne_output_folder = \"embeddings\" + ne_jar + \"-\" + llr_params_serialization + \"-\" + today + \"-\" + embs + n2v\n",
    "    output_for_embeddings = l2v_cfg['PATHS'][\"OUTPUT\"] + \"network-embeddings/\" + ne_output_folder\n",
    "    \n",
    "    embedding_JAR = l2v_cfg['PATHS'][\"JARS\"] + l2v_cfg['JARS'][\"EMBEDDINGS_JAR\"]\n",
    "\n",
    "    embeddings_d = l2v_params['EMBEDDINGS'][\"dim\"]\n",
    "\n",
    "    w = l2v_params['EMBEDDINGS'][\"window\"]\n",
    "    l = l2v_params['EMBEDDINGS'][\"walkLength\"]\n",
    "    n = l2v_params['EMBEDDINGS'][\"numWalks\"]\n",
    "    de = l2v_params['EMBEDDINGS'][\"degree\"]\n",
    "    p = l2v_params['EMBEDDINGS'][\"p\"]\n",
    "    q = l2v_params['EMBEDDINGS'][\"q\"]\n",
    "\n",
    "    network_embeddings_EMR = \"\"\"spark-submit --deploy-mode cluster --class Main {} --dim {} --window {} --walkLength {} --numWalks {} --degree {} --p {} --q {} --weighted true --directed false --indexed true --input {} --output {} --cmd node2vec\"\"\".format(embedding_JAR, embeddings_d, w, l, n, de, p, q, input_embeddings, output_for_embeddings )\n",
    "    \n",
    "    return network_embeddings_EMR, output_for_embeddings, embs, n2v, embeddings_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_PRED_cli(l2v_params, l2v_cfg, output_for_embeddings, llr_params_serialization, embs, n2v, embeddings_d):\n",
    "    today = datetime.datetime.now().strftime(\"%m%d%y\")\n",
    "    \n",
    "    prediction_JAR = l2v_cfg[\"PATHS\"][\"JARS\"] + l2v_cfg[\"JARS\"][\"PREDICTIONS_JAR\"]\n",
    "    p_ntype = l2v_params[\"PREDICTIONS\"][\"ntype\"]\n",
    "    p_neighbors = l2v_params[\"PREDICTIONS\"][\"neighbors\"]\n",
    "    emb_path = output_for_embeddings + \".emb\" + \"/part-00000\"\n",
    "    p_output = \"-\" + str(p_neighbors) + \"-\" + today\n",
    "#     p_output_folder = llr_params_serialization + \"-\" + embs[:4] + n2v + \"-\" + str(p_neighbors) + \"-\" + today\n",
    "    p_output_folder = llr_params_serialization + \"-\" + embs + n2v + \"-\" + str(p_neighbors) + \"-\" + today\n",
    "    prediction_path = l2v_cfg[\"PATHS\"][\"OUTPUT\"] + \"predictions/\" + p_output_folder\n",
    "    rmse_path = l2v_cfg[\"PATHS\"][\"OUTPUT\"] + \"rmse/\" + p_output_folder\n",
    "    prediction_EMR = \"\"\"spark-submit --deploy-mode cluster --class Prediction --master yarn-cluster {} --dim {} --ntype {} --train {} --test {} --embedding {} --neighbors {} --rmse {} --predictions {}\"\"\".format(prediction_JAR, embeddings_d, p_ntype, l2v_cfg[\"DATA\"][\"TRAIN\"], l2v_cfg[\"DATA\"][\"VALIDATION\"], emb_path, p_neighbors, rmse_path, prediction_path)\n",
    "    return prediction_EMR, p_output_folder, prediction_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Create function for evaluation\n",
    "\n",
    "\n",
    "\n",
    "# options: rmse, map, allMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_EVAL_cli(l2v_cfg, l2v_params, p_output_folder, prediction_path):\n",
    "    \n",
    "    evaluation_JAR = l2v_cfg[\"PATHS\"][\"JARS\"] + l2v_cfg[\"JARS\"][\"EVALUATION_JAR\"]\n",
    "\n",
    "    options = l2v_params[\"EVALUATION\"][\"options\"]\n",
    "\n",
    "    inputFile = prediction_path\n",
    "\n",
    "    outputFile = l2v_cfg[\"PATHS\"][\"OUTPUT\"] + \"eval/\" + p_output_folder\n",
    "\n",
    "    evaluation_EMR = \"\"\"spark-submit --deploy-mode cluster --class eval --master yarn {} --options {} --inputFile {} --outputFile {}\"\"\".format(evaluation_JAR,options,inputFile,outputFile)\n",
    "    \n",
    "    return evaluation_EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params_to_cli(path_to_l2v_config, path_to_l2v_params):\n",
    "    # load params\n",
    "    l2v_cfg, l2v_params = load_configs_params(path_to_l2v_config, path_to_l2v_params)\n",
    "    # llr command\n",
    "    llr, output_folder_LLR, llr_params_serialization = write_LLR_cli(l2v_params, l2v_cfg) \n",
    "    # embeddings command\n",
    "    emb, output_for_embeddings, embs, n2v, embeddings_d = write_EMB_cli(l2v_params, l2v_cfg, output_folder_LLR, llr_params_serialization)\n",
    "    # prediction command\n",
    "    pred, p_output_folder, prediction_path = write_PRED_cli(l2v_params, l2v_cfg, output_for_embeddings, llr_params_serialization, embs, n2v, embeddings_d)\n",
    "    # evaluation command\n",
    "    evaluation = write_EVAL_cli(l2v_cfg, l2v_params, p_output_folder, prediction_path)\n",
    "    \n",
    "    return llr, emb, pred, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr, emb, pred, evaluation = params_to_cli(\"CONFIGS/ex1-ml-1m-config.yml\", \"CONFIGS/ex3420-du05d100w10l80n10d30p5q1-900-072717-params.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these 3 variables contain the AWS CLI parameters for each of the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark-submit --deploy-mode cluster --class llr.LLR s3://sandbox-l2v/JARs/llr-assembly-1.2.jar --master yarn --options default --useroritem user --threshold 0.5 --interactionsFile s3://sandbox-l2v/datasets/ml-1m/split/split-cleaned-formatted-4and5/ml1m-train-clean4and5 --outputFile s3://sandbox-l2v/datasets/ml-1m/llr_output/llr12-081417-du05 --separator \",\" --maxInteractionsPerUserOrItem 500 --seed 12345'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark-submit --deploy-mode cluster --class Main s3://sandbox-l2v/JARs/n2v-assembly-3.7.jar --dim 100 --window 10 --walkLength 80 --numWalks 10 --degree 30 --p 5 --q 1 --weighted true --directed false --indexed true --input s3://sandbox-l2v/datasets/ml-1m/llr_output/llr12-081417-du05/part-00000 --output s3://sandbox-l2v/datasets/ml-1m/network-embeddings/embeddings37-du05-081417-d100w10l80n10d30-p5q1 --cmd node2vec'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark-submit --deploy-mode cluster --class Prediction --master yarn-cluster s3://sandbox-l2v/JARs/prediction-assembly-2.2.jar --dim 100 --ntype KNN --train s3://sandbox-l2v/datasets/ml-1m/split/split-cleaned-formatted-4and5/ml1m-train-clean4and5 --test s3://sandbox-l2v/datasets/ml-1m/split/split-cleaned-formatted/ml1m-validation-clean --embedding s3://sandbox-l2v/datasets/ml-1m/network-embeddings/embeddings37-du05-081417-d100w10l80n10d30-p5q1.emb/part-00000 --neighbors 900 --rmse s3://sandbox-l2v/datasets/ml-1m/rmse/du05-d100w10l80n10d30-p5q1-900-081417 --predictions s3://sandbox-l2v/datasets/ml-1m/predictions/du05-d100w10l80n10d30-p5q1-900-081417'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark-submit --deploy-mode cluster --class eval --master yarn s3://sandbox-l2v/JARs/evaluation-assembly-1.1.jar --options allMetrics --inputFile s3://sandbox-l2v/datasets/ml-1m/predictions/du05-d100w10l80n10d30-p5q1-900-081417 --outputFile s3://sandbox-l2v/datasets/ml-1m/eval/du05-d100w10l80n10d30-p5q1-900-081417'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next step is to load this steps using BOTO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
